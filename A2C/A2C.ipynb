{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799a239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "import ale_py   \n",
    "import gymnasium as gym\n",
    "from gymnasium.wrappers import AtariPreprocessing, FrameStack\n",
    "from torch.distributions import Categorical\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0e438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# 1. Hyper-parameters & configuration\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "class Config:\n",
    "    # Environment\n",
    "    env_id: str = \"ALE/MsPacman-v5\"\n",
    "    seed: int = 42\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # A2C\n",
    "    epsilon: float = 0.05\n",
    "    gamma: float = 0.99\n",
    "    rollout_steps: int = 5\n",
    "    total_episodes: int = 10_000\n",
    "\n",
    "    # Optimisation\n",
    "    lr: float = 1e-4\n",
    "    entropy_coef: float = 0.05\n",
    "    value_loss_coef: float = 0.5        \n",
    "    grad_clip_norm: float = 0.5         \n",
    "\n",
    "    # Logging\n",
    "    log_interval: int = 1             \n",
    "    moving_avg_episodes: int = 100\n",
    "\n",
    "    # DEBUG \n",
    "    debug_interval: int = 10\n",
    "    \n",
    "\n",
    "# Reproducibility\n",
    "random.seed(Config.seed)\n",
    "np.random.seed(Config.seed)\n",
    "torch.manual_seed(Config.seed)\n",
    "if Config.device.type == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(Config.seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d4c243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# 2. Helpers – environment factory\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def make_env(env_id: str, seed: int | None = None) -> gym.Env:\n",
    "    env = gym.make(\n",
    "        env_id,\n",
    "        frameskip=1,\n",
    "        full_action_space=False\n",
    "    )\n",
    "    env = AtariPreprocessing(\n",
    "        env,\n",
    "        frame_skip=4,\n",
    "        screen_size=84,          # smaller than 128 → faster\n",
    "        terminal_on_life_loss=False,\n",
    "        grayscale_obs=True,\n",
    "        scale_obs=True\n",
    "    )\n",
    "    env = FrameStack(env, num_stack=4)\n",
    "    if seed is not None:\n",
    "        env.reset(seed=seed)\n",
    "        env.action_space.seed(seed)\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ab5dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# 3. Neural network – shared conv base + policy/value heads\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "class ActorCriticNet(nn.Module):\n",
    "    def __init__(self, input_shape: tuple[int, int, int], num_actions: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # In FrameStack the shape is (C,H,W) where C ≤ 4\n",
    "        if len(input_shape) != 3:\n",
    "            raise ValueError(\"Expected 3-D input shape\") \n",
    "        channels, height, width = input_shape if input_shape[0] <= 4 else (input_shape[2], *input_shape[:2])\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channels, 32, 8, 4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 4, 2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, 3, 1)\n",
    "\n",
    "        # Compute conv output size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, channels, height, width)\n",
    "            out = self._forward_conv(dummy)\n",
    "            flat = out.view(1, -1).size(1)\n",
    "\n",
    "        self.fc = nn.Linear(flat, 1024)\n",
    "        self.policy_head = nn.Linear(1024, num_actions)\n",
    "        self.value_head = nn.Linear(1024, 1)\n",
    "\n",
    "    def _forward_conv(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        # convert to float and channels-first\n",
    "        x = x.float()\n",
    "        if x.ndim == 4 and x.shape[1] not in (1, 3, 4):\n",
    "            # channels last → channels first\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        if x.max() > 1.0:\n",
    "            x = x / 255.0\n",
    "        x = self._forward_conv(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc(x))\n",
    "        logits = self.policy_head(x)\n",
    "        value = self.value_head(x)\n",
    "        return logits, value.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67c6e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# 4. A2C agent\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "class A2CAgent:\n",
    "    def __init__(self, model: nn.Module, cfg: Config):\n",
    "        self.model = model\n",
    "        self.device = cfg.device\n",
    "        self.gamma = cfg.gamma\n",
    "        self.entropy_coef = cfg.entropy_coef\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state_t = torch.tensor(state, device=self.device)\n",
    "        if state_t.ndim == 3:\n",
    "            state_t = state_t.unsqueeze(0)\n",
    "        logits, value = self.model(state_t)\n",
    "        dist = Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        if random.random() < Config.epsilon:\n",
    "            action = torch.tensor(dist.sample().new_full((), random.randrange(dist.probs.size(-1))))\n",
    "        return (\n",
    "            action.item(),\n",
    "            dist.log_prob(action).squeeze(0),\n",
    "            value.squeeze(0),\n",
    "            dist.entropy().squeeze(0)\n",
    "        )\n",
    "\n",
    "    def update(self, rewards, log_probs, values, entropies, next_value, done):\n",
    "        \"\"\"Performs the A2C update and **returns debug scalars**.\"\"\"\n",
    "        rewards = torch.tensor(rewards, device=self.device, dtype=torch.float32)\n",
    "        log_probs = torch.stack(log_probs)\n",
    "        values = torch.stack(values)        # shape [T]\n",
    "        entropies = torch.stack(entropies)\n",
    "\n",
    "        # Compute returns\n",
    "        R = 0.0 if done else next_value.item()\n",
    "        returns = []\n",
    "        for r in reversed(rewards):\n",
    "            R = r + self.gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns, device=self.device)\n",
    "\n",
    "        advantages = returns - values.detach()\n",
    "        adv_mean = advantages.mean()\n",
    "        adv_std  = advantages.std(unbiased=False)        # never NaN for 1 element\n",
    "        advantages = (advantages - adv_mean) / (adv_std + 1e-8)\n",
    "\n",
    "        # Losses\n",
    "        policy_loss = -(advantages * log_probs).mean()\n",
    "        value_loss = F.mse_loss(values, returns)\n",
    "        entropy_mean = entropies.mean()\n",
    "        total_loss = (policy_loss\n",
    "              + Config.value_loss_coef * value_loss       # critic weight ↓\n",
    "              - Config.entropy_coef      * entropies.mean())\n",
    "\n",
    "        # Back-prop\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), Config.grad_clip_norm)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # DEBUG scalars (Python floats)\n",
    "        return (\n",
    "            policy_loss.item(),\n",
    "            value_loss.item(),\n",
    "            entropy_mean.item(),\n",
    "            total_loss.item(),\n",
    "            advantages.mean().item(),\n",
    "            advantages.min().item(),\n",
    "            advantages.max().item()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e01f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# 5. Training loop with integrated debugging\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    env = make_env(Config.env_id, Config.seed)\n",
    "    obs_shape = env.observation_space.shape      # (C, H, W) after preprocessing\n",
    "    n_actions = env.action_space.n\n",
    "\n",
    "    model = ActorCriticNet(obs_shape, n_actions).to(Config.device)\n",
    "    agent = A2CAgent(model, Config)\n",
    "\n",
    "    episode_rewards = deque(maxlen=Config.moving_avg_episodes)\n",
    "\n",
    "    # DEBUG accumulators\n",
    "    debug_on = bool(Config.debug_interval)\n",
    "    interval = Config.debug_interval or 1  # avoid div by zero\n",
    "\n",
    "    act_counts_interval = np.zeros(n_actions, dtype=int)\n",
    "    pl_sum = vl_sum = ent_sum = tot_sum = adv_mean_sum = 0.0\n",
    "    adv_min_list, adv_max_list = [], []\n",
    "\n",
    "    for ep in range(1, Config.total_episodes + 1):\n",
    "        state, _ = env.reset()\n",
    "        done, total_reward = False, 0.0\n",
    "        # Per-episode action counts (for optional per-episode analysis)\n",
    "        ep_act_counts = np.zeros(n_actions, dtype=int)\n",
    "\n",
    "        # Roll out until done\n",
    "        while not done:\n",
    "            rewards, log_probs, values, entropies = [], [], [], []\n",
    "\n",
    "            # n-step rollout (or until episode ends)\n",
    "            for _ in range(Config.rollout_steps):\n",
    "                action, logprob, value, entropy = agent.select_action(state)\n",
    "                next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                rewards.append(reward)\n",
    "                log_probs.append(logprob)\n",
    "                values.append(value)\n",
    "                entropies.append(entropy)\n",
    "                total_reward += reward\n",
    "                ep_act_counts[action] += 1\n",
    "\n",
    "                state = next_state\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            # bootstrap value\n",
    "            if done:\n",
    "                next_val = torch.zeros(1, device=Config.device)\n",
    "            else:\n",
    "                st_t = torch.tensor(state, device=Config.device)\n",
    "                if st_t.ndim == 3:\n",
    "                    st_t = st_t.unsqueeze(0)\n",
    "                with torch.no_grad():\n",
    "                    _, next_val_t = agent.model(st_t)\n",
    "                next_val = next_val_t.squeeze(0)\n",
    "\n",
    "            # agent update & debug scalars\n",
    "            pl, vl, ent, tot, adv_m, adv_min, adv_max = agent.update(\n",
    "                rewards, log_probs, values, entropies, next_val, done\n",
    "            )\n",
    "\n",
    "        # ─── end of episode ────────────────────────────────────────────\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # accumulate debug stats\n",
    "        if debug_on:\n",
    "            pl_sum += pl\n",
    "            vl_sum += vl\n",
    "            ent_sum += ent\n",
    "            tot_sum += tot\n",
    "            adv_mean_sum += adv_m\n",
    "            adv_min_list.append(adv_min)\n",
    "            adv_max_list.append(adv_max)\n",
    "            act_counts_interval += ep_act_counts\n",
    "\n",
    "        # reward log\n",
    "        if ep % Config.log_interval == 0:\n",
    "            avg_r = sum(episode_rewards) / len(episode_rewards)\n",
    "            print(f\"Episode {ep:<4d} | EpReward: {total_reward:>6.2f} | MovingAvg({len(episode_rewards)}): {avg_r:6.2f}\")\n",
    "\n",
    "        # DEBUG print every N episodes\n",
    "        if debug_on and ep % interval == 0:\n",
    "            avg_pl  = pl_sum  / interval\n",
    "            avg_vl  = vl_sum  / interval\n",
    "            avg_ent = ent_sum / interval\n",
    "            avg_tot = tot_sum / interval\n",
    "            avg_adv_mean = adv_mean_sum / interval\n",
    "            adv_min_int = min(adv_min_list)\n",
    "            adv_max_int = max(adv_max_list)\n",
    "\n",
    "            total_act = act_counts_interval.sum()\n",
    "            if total_act:\n",
    "                pct = act_counts_interval / total_act * 100.0\n",
    "                act_dist = \", \".join(f\"{i}:{c} ({p:.1f}%)\" for i, (c, p) in enumerate(zip(act_counts_interval, pct)))\n",
    "            else:\n",
    "                act_dist = \"n/a\"\n",
    "\n",
    "            print(f\"\\n--- DEBUG {ep - interval + 1}-{ep} ---\")\n",
    "            print(f\"Avg PolicyLoss: {avg_pl:.4f} | Avg ValueLoss: {avg_vl:.4f} | \"\n",
    "                  f\"Avg Entropy: {avg_ent:.4f} | Avg TotalLoss: {avg_tot:.4f}\")\n",
    "            print(f\"Advantage μ: {avg_adv_mean:.4f} | min: {adv_min_int:.4f} | max: {adv_max_int:.4f}\")\n",
    "            print(f\"Action distribution (last {interval} eps): {act_dist}\\n\")\n",
    "\n",
    "            # reset accumulators\n",
    "            pl_sum = vl_sum = ent_sum = tot_sum = adv_mean_sum = 0.0\n",
    "            adv_min_list.clear()\n",
    "            adv_max_list.clear()\n",
    "            act_counts_interval.fill(0)\n",
    "\n",
    "    # closing the environment\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd85e07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "# 6. Execution\n",
    "# ──────────────────────────────────────────────────────────────────────────\n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
