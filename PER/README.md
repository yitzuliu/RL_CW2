# Prioritized Experience Replay (PER) for Atari Games

Training a Deep Q-Network (DQN) to play Atari games using Prioritized Experience Replay (PER) technique.

## ğŸ“ Project Overview

This project implements a Deep Q-Network (DQN) with Prioritized Experience Replay (PER) to play Atari games. PER is an enhanced experience replay mechanism that prioritizes sampling of high-value experiences based on their importance (measured by TD-error). This approach significantly improves DQN's learning efficiency and performance.
## ğŸ› ï¸ Installation & Setup

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- Gymnasium (newer version of OpenAI Gym)
- NumPy, Matplotlib, OpenCV
- Other dependencies listed in requirements.txt

## ğŸ“Š Project Structure

```
.
â”œâ”€â”€ config.py                    # Configuration file with all hyperparameters
â”œâ”€â”€ train.py                     # Training script to start the training process
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dqn_agent.py             # DQN agent implementation
â”‚   â”œâ”€â”€ per_memory.py            # Prioritized Experience Replay memory
â”‚   â”œâ”€â”€ sumtree.py               # SumTree data structure implementation
â”‚   â”œâ”€â”€ q_network.py             # Q-Network neural network architecture
â”‚   â”œâ”€â”€ env_wrappers.py          # Atari environment wrappers
â”‚   â”œâ”€â”€ device_utils.py          # Device detection and optimization utilities
â”‚   â”œâ”€â”€ logger.py                # Training logging tools
â”‚   â””â”€â”€ visualization.py         # Training metrics visualization tools
â”œâ”€â”€ results/                      # Directory for storing training results
â”‚   â”œâ”€â”€ data/                    # Training data
â”‚   â”œâ”€â”€ logs/                    # Training logs
â”‚   â”œâ”€â”€ models/                  # Saved models
â”‚   â””â”€â”€ plots/                   # Generated plots
â””â”€â”€ CHECKLIST.md                 # Implementation plan and pseudocode
```

## ğŸš€ Usage

### Training a Model

To train a new model from scratch:

```bash
python train.py
```

### Algorithm Description

This project implements the DQN algorithm with Prioritized Experience Replay:

1. **Prioritized Experience Replay (PER)**:
    - Efficiently stores and samples experiences using a SumTree data structure
    - Transitions are assigned priorities based on TD-error: p = (|Î´|+Îµ)^Î±
    - Importance sampling weights w = (NÂ·P(i))^(-Î²) correct the bias introduced
    - Î² gradually increases from 0.4 to 1.0 over time

2. **DQN Architecture**:
    - 3-layer convolutional neural network followed by fully connected layers
    - Dual network architecture (policy and target networks) with PER integration
    - Îµ-greedy exploration strategy with decaying Îµ value
    - Target network updates every 20,000 steps

## ğŸ” Algorithm Details

### SumTree Data Structure

The implemented SumTree has the following main operations:
- `add`: Add new experience with its priority
- `update_priority`: Update the priority of an experience
- `get_experience_by_priority`: Retrieve experience based on priority value

### Prioritized Experience Replay Mechanism

The PER memory implements:
- Priority calculation based on TD-errors
- Computation and proper application of importance sampling weights
- Linear annealing strategy for Î² value
- Efficient batch sampling